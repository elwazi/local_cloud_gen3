networks: {devnet: null}
services:
  arborist-service:
    container_name: arborist-service
    depends_on: [postgres]
    entrypoint: bash /go/src/github.com/uc-cdis/arborist/arborist_setup.sh
    environment: ['JWKS_ENDPOINT=http://fence-service/.well-known/jwks', PGDATABASE=arborist_db,
      PGUSER=arborist_user, PGPASSWORD=arborist_pass, PGHOST=postgres, PGPORT=5432,
      PGSSLMODE=disable]
    healthcheck:
      interval: 60s
      retries: 10
      test: [CMD-SHELL, 'curl -f http://localhost/health']
      timeout: 5s
    image: quay.io/cdis/arborist:2021.03
    networks: [devnet]
    volumes: ['./scripts/arborist_setup.sh:/go/src/github.com/uc-cdis/arborist/arborist_setup.sh']
  esproxy-service:
    command: [-c, 'echo -e ''cluster.name: docker-cluster

        http.host: 0.0.0.0

        index.store.type: niofs'' > /usr/share/elasticsearch/config/elasticsearch.yml
        && /usr/local/bin/docker-entrypoint.sh eswrapper']
    container_name: esproxy-service
    entrypoint: [/bin/bash]
    environment: [cluster.name=elasticsearch-cluster, bootstrap.memory_lock=false,
      ES_JAVA_OPTS=-Xms1g -Xmx1g]
    healthcheck:
      interval: 60s
      retries: 10
      test: [CMD-SHELL, wait_for_esproxy.sh]
      timeout: 5s
    image: quay.io/cdis/elasticsearch-oss:6.8.12
    networks: [devnet]
    ports: ['9200:9200', '9300:9300']
    ulimits:
      memlock: {hard: -1, soft: -1}
      nofile: {hard: 65536, soft: 65536}
    volumes: ['./scripts/wait_for_esproxy.sh:/usr/bin/wait_for_esproxy.sh:ro', 'esdata:/usr/share/elasticsearch/data']
  fence-service:
    command: bash /var/www/fence/fence_setup.sh
    container_name: fence-service
    depends_on: [postgres, arborist-service]
    environment: [PYTHONPATH=/var/www/fence, GEN3_DEBUG=True]
    healthcheck:
      interval: 60s
      retries: 3
      test: [CMD-SHELL, 'curl -f http://localhost/_status']
      timeout: 5s
    image: quay.io/cdis/fence:2021.03
    networks: [devnet]
    volumes: ['./Secrets/fence-config.yaml:/var/www/fence/fence-config.yaml', './Secrets/user.yaml:/var/www/fence/user.yaml',
      './Secrets/TLS/service.crt:/usr/local/share/ca-certificates/cdis-ca.crt', './Secrets/fenceJwtKeys:/fence/keys',
      './scripts/fence_setup.sh:/var/www/fence/fence_setup.sh']
  guppy-service:
    command: node --max-http-header-size 16000 dist/server/server.js
    container_name: guppy-service
    depends_on: [arborist-service, esproxy-service]
    entrypoint: /usr/bin/wait_for_esproxy.sh
    environment: [GUPPY_CONFIG_FILEPATH=/guppy/guppy_config.json, 'GEN3_ARBORIST_ENDPOINT=http://arborist-service',
      'GEN3_ES_ENDPOINT=http://esproxy-service:9200']
    image: quay.io/cdis/guppy:2021.03
    networks: [devnet]
    volumes: ['./Secrets/guppy_config.json:/guppy/guppy_config.json', './scripts/wait_for_esproxy.sh:/usr/bin/wait_for_esproxy.sh:ro']
  indexd-service:
    command: bash indexd_setup.sh
    container_name: indexd-service
    depends_on: [postgres]
    healthcheck:
      interval: 60s
      retries: 3
      test: [CMD-SHELL, 'curl -f http://localhost/_status']
      timeout: 5s
    image: quay.io/cdis/indexd:2021.03
    networks: [devnet]
    volumes: ['./Secrets/indexd_settings.py:/var/www/indexd/local_settings.py', './Secrets/indexd_creds.json:/var/www/indexd/creds.json',
      './Secrets/config_helper.py:/var/www/indexd/config_helper.py', './scripts/indexd_setup.sh:/var/www/indexd/indexd_setup.sh']
  jupyter-service:
    command: [--NotebookApp.base_url=/lw-workspace/proxy, --NotebookApp.password='',
      --NotebookApp.token='']
    container_name: jupyter-service
    entrypoint: [start-notebook.sh]
    environment: ['FRAME_ANCESTORS=http://localhost']
    image: quay.io/cdis/jupyter-slim:latest
    networks: [devnet]
  kibana-service:
    container_name: kibana-service
    depends_on: [esproxy-service]
    environment: [SERVER_NAME=kibana-service, 'ELASTICSEARCH_URL=http://esproxy-service:9200']
    image: quay.io/cdis/kibana-oss:6.5.4
    networks: [devnet]
    ports: ['5601:5601']
  metadata-service:
    command: 'sh -c "/env/bin/alembic upgrade head && /env/bin/uvicorn --host 0.0.0.0
      --port 80 mds.asgi:app --reload"

      '
    container_name: metadata-service
    depends_on: [postgres]
    environment: [DB_HOST=postgres, DB_USER=metadata_user, DB_PASSWORD=metadata_pass,
      DB_DATABASE=metadata]
    image: quay.io/cdis/metadata-service:2021.03
    networks: [devnet]
  peregrine-service:
    container_name: peregrine-service
    depends_on: [postgres, sheepdog-service]
    environment: {DICTIONARY_URL: 'https://s3.amazonaws.com/dictionary-artifacts/datadictionary/develop/schema.json',
      REQUESTS_CA_BUNDLE: /etc/ssl/certs/ca-certificates.crt}
    healthcheck:
      interval: 60s
      retries: 10
      test: [CMD-SHELL, 'curl -f http://localhost/_status']
      timeout: 5s
    image: quay.io/cdis/peregrine:2021.03
    networks: [devnet]
    volumes: ['./Secrets/peregrine_settings.py:/var/www/peregrine/wsgi.py', './Secrets/peregrine_creds.json:/var/www/peregrine/creds.json',
      './Secrets/config_helper.py:/var/www/peregrine/config_helper.py', './Secrets/TLS/service.crt:/usr/local/share/ca-certificates/cdis-ca.crt',
      './scripts/peregrine_setup.sh:/peregrine_setup.sh', './datadictionary/gdcdictionary/schemas:/schemas_dir']
  pidgin-service:
    container_name: pidgin-service
    depends_on: [peregrine-service]
    healthcheck:
      interval: 60s
      retries: 3
      test: [CMD-SHELL, 'curl -f http://localhost/_status']
      timeout: 5s
    image: quay.io/cdis/pidgin:2021.03
    networks: [devnet]
    volumes: ['./scripts/waitForContainers.sh:/var/www/data-portal/waitForContainers.sh']
  portal-service:
    command: [bash, /var/www/data-portal/waitForContainers.sh]
    container_name: portal-service
    depends_on: [postgres, peregrine-service, sheepdog-service]
    environment: [NODE_ENV=dev, APP=gitops, 'GDC_SUBPATH=http://revproxy-service/api/v0/submission/']
    healthcheck:
      interval: 60s
      retries: 10
      test: [CMD-SHELL, 'curl -f http://localhost']
      timeout: 5s
    image: quay.io/cdis/data-portal:2021.03
    networks: [devnet]
    volumes: ['./scripts/waitForContainers.sh:/var/www/data-portal/waitForContainers.sh',
      './Secrets/gitops.json:/data-portal/data/config/gitops.json', './Secrets/gitops-logo.png:/data-portal/custom/logo/gitops-logo.png',
      './Secrets/gitops.png:/data-portal/custom/createdby/gitops.png']
  postgres: {}
  revproxy-service:
    container_name: revproxy-service
    depends_on: [arborist-service, indexd-service, peregrine-service, sheepdog-service,
      fence-service, portal-service, pidgin-service]
    healthcheck:
      interval: 60s
      retries: 3
      test: [CMD-SHELL, 'curl -f http://localhost/_status']
      timeout: 5s
    image: quay.io/cdis/nginx:2021.03
    networks: [devnet]
    ports: ['80:80', '443:443']
    volumes: ['./nginx.conf:/etc/nginx/nginx.conf', './Secrets/TLS/service.crt:/etc/nginx/ssl/nginx.crt',
      './Secrets/TLS/service.key:/etc/nginx/ssl/nginx.key']
  sheepdog-service:
    command: bash /sheepdog_setup.sh
    container_name: sheepdog-service
    depends_on: [postgres]
    environment: {DICTIONARY_URL: 'https://s3.amazonaws.com/dictionary-artifacts/datadictionary/develop/schema.json',
      REQUESTS_CA_BUNDLE: /etc/ssl/certs/ca-certificates.crt}
    healthcheck:
      interval: 60s
      retries: 5
      test: [CMD-SHELL, 'curl -f http://localhost/_status']
      timeout: 5s
    image: quay.io/cdis/sheepdog:2021.03
    networks: [devnet]
    volumes: ['./Secrets/sheepdog_settings.py:/var/www/sheepdog/wsgi.py', './Secrets/sheepdog_creds.json:/var/www/sheepdog/creds.json',
      './Secrets/config_helper.py:/var/www/sheepdog/config_helper.py', './scripts/sheepdog_setup.sh:/sheepdog_setup.sh',
      './datadictionary/gdcdictionary/schemas:/schemas_dir']
  spark-service:
    command: bash -c "python run_config.py && hdfs namenode -format && hdfs --daemon
      start namenode && hdfs --daemon start datanode && yarn --daemon start resourcemanager
      && yarn --daemon start nodemanager && hdfs dfsadmin -safemode leave &&  hdfs
      dfs -mkdir /result && while true; do sleep 5; done"
    container_name: spark-service
    environment: ['HADOOP_URL=hdfs://0.0.0.0:9000', HADOOP_HOST=0.0.0.0]
    expose: [22, 8030, 8031, 8032, 9000]
    image: quay.io/cdis/gen3-spark:2021.03
    networks: [devnet]
  tube-service:
    command: bash -c "while true; do sleep 5; done"
    container_name: tube-service
    depends_on: [postgres, esproxy-service, spark-service]
    environment: ['DICTIONARY_URL=https://s3.amazonaws.com/dictionary-artifacts/datadictionary/develop/schema.json',
      ES_URL=esproxy-service, ES_INDEX_NAME=etl, 'HADOOP_URL=hdfs://spark-service:9000',
      HADOOP_HOST=spark-service]
    image: quay.io/cdis/tube:2021.03
    networks: [devnet]
    volumes: ['./Secrets/etl_creds.json:/usr/share/gen3/tube/creds.json', './Secrets/etlMapping.yaml:/usr/share/gen3/tube/etlMapping.yaml',
      './Secrets/user.yaml:/usr/share/gen3/tube/user.yaml', './datadictionary:/tmp/datadictionary']
version: '3'
volumes: {esdata: null, psqldata: null}
